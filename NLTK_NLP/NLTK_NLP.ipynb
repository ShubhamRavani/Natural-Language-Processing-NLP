{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.chunk.api import ChunkParserI\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "import subprocess\n",
    "from nltk.book import text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12182a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "f = open(\"story.txt\", \"r\")\n",
    "text = f.read()\n",
    "\n",
    "# Sent Tokenization\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad569260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "# Word Tokenization\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121016b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Case Conversion\n",
    "\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "words = text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebe485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word\n",
    "\n",
    "f = open(\"story.txt\")\n",
    "text = f.read()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words_in_quote = word_tokenize(text)\n",
    "\n",
    "filtered_list = [word for word in words_in_quote if word.casefold() not in stop_words]\n",
    "filtered_list = []\n",
    "\n",
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "        \n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aec051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "f = open(\"story.txt\")\n",
    "text = f.read()\n",
    "\n",
    "words = word_tokenize(text)\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = word_tokenize(text)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "words = word_tokenize(text)\n",
    "lotr_pos_tags = nltk.pos_tag(words)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(lotr_pos_tags)\n",
    "\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinking\n",
    "\n",
    "words = word_tokenize(text)\n",
    "lotr_pos_tags = nltk.pos_tag(words)\n",
    "grammar = \"\"\"\n",
    "    Chunk: {<.*>+}\n",
    "           }<JJ>{\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "tree = chunk_parser.parse(lotr_pos_tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e75308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Named Entity Recognition (NER)\n",
    "\n",
    "f = open(\"story.txt\")\n",
    "\n",
    "def extract_ne(text):\n",
    "    words = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tree = nltk.ne_chunk(tags, binary=True)\n",
    "    tree.draw()\n",
    "    \n",
    "\n",
    "\n",
    "text = f.read()\n",
    "extract_ne(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e20562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a Dispersion Plot\n",
    "\n",
    "f = open(\"story.txt\")\n",
    "text = f.read()\n",
    "words = word_tokenize(text)\n",
    "text8.dispersion_plot(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a Frequency Distribution\n",
    "\n",
    "meaningful_words = [word for word in words if word.casefold() not in stop_words]\n",
    "frequency_distribution = FreqDist(meaningful_words)\n",
    "frequency_distribution.plot(20, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse tree or Syntax Tree generation\n",
    "\n",
    "f = open(\"story.txt\")\n",
    "text = f.read()\n",
    "tagged = pos_tag(word_tokenize(text))\n",
    "chunker = RegexpParser(\"\"\" \n",
    "NP: {<.*>*}   \n",
    "}<[\\.VI].*>+{       \n",
    "<.*>}{<DT>        \n",
    "PP: {<IN><NP>}        \n",
    "VP: {<VB.*><NP|PP>*}\n",
    "\"\"\" )\n",
    "output = chunker.parse(tagged)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba04eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse tree or Syntax Tree generation\n",
    "\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "\n",
    "f = open(\"story.txt\")\n",
    "text = f.read()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized = sent_tokenize(text)\n",
    "\n",
    "for i in tokenized:\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)\n",
    "    output = chunker.parse(tagged)\n",
    "    output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fa485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
